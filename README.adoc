= Silver Broccoli: Spring Cloud Stream (SCS) + Apache Kafka Evaluation

* [x] Document how to connect to a RHOAS Kafka instance.
* [x] Auto configure a topic.
* [x] Generate messages into the topic on a schedule.
* [x] Enqueue a message in response to arbitrary external stimuli.
* [x] Use a structured message type (A jackson-serialized POJO)
* [x] Enqueue a message as a consequence of processing another.
** [ ] Produce multiple messages.
* [x] Consume messages from one topic concurrently.
* [x] Demonstrate re-tried message consumption.
* [x] Demonstrate bounded re-try so that unprocessable entities do not block the topic.
* [ ] Create an idempotent message consumer
* [x] Enable DLQ for a topic.
**  [x] Demonstrate how to consume from a DLQ.
**  [ ] Find or build a simple CLI to inspect DLQ entries.
* [ ] Consume a batch of messages.
* [ ] Prioritize message consumption within a topic.
* [ ] Research monitoring options

Additional questions:

* [ ] What is the default message key used for partition selection?
* [ ] How do you change the default encoding of DLQ entries? The default is byte-arrays for values.

== Documentation Links

Refer to the following:

* link:https://github.com/spring-cloud/spring-cloud-stream-samples[Sample S.C.S. projects].
* link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-reference[Spring Cloud Stream]
* link:https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_apache_kafka_binder[Apache Kafka Binder for Spring Cloud Stream]
* link:https://kafka.apache.org/documentation/#configuration[Kafka configuration properties] (see topic and producer configs).

== Development Setup

For local development you may either perform integrated testing in a docker-compose environment or with an externally deployed Kafka instance using the `sasl` profile.

=== Docker Compose
In this section we will launch and integrate with a local instance of Kafka using docker-compose.

In one terminal, start Kafka and Zookeeper using docker-compose:

[source,sh]
----
docker-compose up
----

In another terminal, launch the application with desired profiles:

[source,sh]
----
SPRING_PROFILES_ACTIVE=numbers ./mvnw spring-boot:run
----

=== External Kafka Integration (With SASL Authentication)
In this section we will run the application from maven and integrate with an external kafka instance using SSL-encrypted SASL/PLAIN (username and password) authentication. We will inject SASL configuration into the application environment using an untracked application profile, but we could also set environment variables with the same names.

To begin, create a `config/application-local.yml` file. This file will hold custom environment configuration and secrets which we do not want to commit to the repository. This file is already git-ignored. Add the following required fields:

[source,yml]
----
env:
  kafka:
    bootstrapservers: <broker connection string, e.g. hostname:port>
    securityprotocol: SASL_SSL
    saslmechanism: PLAIN
    user: <service account user name>
    password: <service account password>
----

Now launch the application with both the `sasl` and `local` profiles enabled. The `local` profile will simply read the application-local.yml file. The `sasl` profile maps those configurations to appropriate Kafka and SASL configurations.

NOTE: If we did not want to use an `application-local.yml` file, we could set environment properties of corresponding names (e.g. `env.kafka.bootstrapservers=localhost:1234`). In this case, we would not need to set the `local` profile.

To launch the application, invoke the following command:

[source,sh]
----
SPRING_PROFILES_ACTIVE=local,sasl ./mvnw spring-boot:run
----

== Evaluation Notes

=== Topic Configuration

We can manually set up topics on an external broker, or we can automatically provision them when the application starts. The latter is controlled by binding configurations like the following:

[source,yml]
----
spring:
  cloud:
    stream:
      bindings:
        producer-out-0:
          destination: numbers
          producer:
            # Provision new topics with 3 partitions. This configuration only
            # applies to producer bindings like this one.
            partitionCount: 3
      # Some producer configurations only apply to kafka, so they are are under
      # the s.c.s.kafka namespace.
      kafka:
        bindings:
          producer-out-0:
            producer:
              # Everything under this key is a native kafka configuration. See
              # below for a link to kafka.apache.org documentation.
              topic.properties:
                # Retain only 10,000 bytes. The default is unlimited (-1).
                retention.bytes: 10000
                # Retain messages for 2 days. The default is 7. -1 is unlimited.
                retention.ms: 172800000
----

Kafka-specific property configuration may be found link:https://kafka.apache.org/documentation/#topicconfigs[here]. Note that new partitions can be added to existing topics, but is disabled by default (see kafka binder documentation for spring.cloud.stream.kafka.binder.autoAddPartitions).

=== Generate And Consume Messages

In this section we produce, transform, and consume messages using the native java functional types `Supplier`, `Function`, and `Consumer`. We will look at the more nuanced Reactor types `Flux` and `Mono` in a separate section.

To generate messages on a short fixed schedule (say, once a second), we can either configure a `Supplier<Flux<T>>` bean or just a `Supplier<T>` bean. The former must manually control the production rate, whereas the latter relies on S.C.S's global polling schedule (see link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#_polling_configuration_properties[polling configuration]). This application registers a `Supplier` to emit numbers to a topic. Find the relevant code in SilverBroccoliApplication.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/NumberHandlers.java[tag=generate-message-with-supplier]
----

To generate messages in response to arbitrary external events, we use an instance of  link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#_sending_arbitrary_data_to_an_output_e_g_foreign_event_driven_sources[StreamBridge]. StreamBridge allows us to enqueue messages one at a time to an arbitrary spring binding, which maps the message to a Kafka topic. This application serves a REST API (`POST /number`) which uses StreamBridge to enqueue a message in response to HTTP requests. Find the relevant code in NumberRestController.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/NumberRestController.java[tag=generate-message-with-streambridge]
----

To consume a message, we register an instance of `Consumer<IN>`. Find the relevant code in SilverBroccoliApplication.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/NumberHandlers.java[tag=consume-message-with-consumer]
----

To process one message and generate another as a result (that is, transform a message), we register an instance of `Function<IN, OUT>`. This is very similar to how we register producers and consumers. Find the relevant code in SilverBroccoliApplication.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/NumberHandlers.java[tag=transform-message-with-function]
----

=== Structured Messages

Spring Cloud Stream supports structures messages. We simply use Jackson-annotated POJOs as message objects. For an example, see NumberMessage.java and NumberHandlers.java.

=== Concurrent Message Consumers

In NumberHandlers.java, we have configured a `debugConsumer` message handler to consume from and display metadata about the `number-structured` topic. The topic has three partitions, and we start two consumer instances. The resulting output looks like the following:

----
2021-08-20 17:29:48.144  INFO 53572 --- [container-0-C-1] c.g.t.s.SilverBroccoliApplication        :
Partition=0
Thread=KafkaConsumerDestination{consumerDestinationName='numbers-structured', partitions=3, dlqName='null'}.container-0-C-1
ConsumerGroup=anonymous.f247e2f0-b12c-488e-9607-b940ef07d236
Payload=NumberMessage{ number=12 }

2021-08-20 17:29:49.145  INFO 53572 --- [container-0-C-1] c.g.t.s.SilverBroccoliApplication        :
Partition=1
Thread=KafkaConsumerDestination{consumerDestinationName='numbers-structured', partitions=3, dlqName='null'}.container-0-C-1
ConsumerGroup=anonymous.f247e2f0-b12c-488e-9607-b940ef07d236
Payload=NumberMessage{ number=13 }

2021-08-20 17:29:50.137  INFO 53572 --- [container-1-C-1] c.g.t.s.SilverBroccoliApplication        :
Partition=2
Thread=KafkaConsumerDestination{consumerDestinationName='numbers-structured', partitions=3, dlqName='null'}.container-1-C-1
ConsumerGroup=anonymous.f247e2f0-b12c-488e-9607-b940ef07d236
Payload=NumberMessage{ number=14 }
----

Above we see that the application (running in one JVM) has consumed three messages from the `numbers-structured` topic. They were consumed from the 0th, 1st, and 2nd partitions of the topic respectively. The thread is printed with each message. Since we're using the Kafka binder, threads are one-to-one with consumers. Since we can see only two threads (0-C-1 and 1-C-1), we know there are only two consumers drawing from all three partitions. This is expected: When a smaller number of consumers than partitions is configured, Kafka will balance some consumers to draw from multiple partitions. Note, however, that at most one consumer from a single consumer group may consume from any partition.

Concurrency configuration for Kafka is simple. The only required parameter is `consumer.concurrency`, which instructs S.C.S. to start multiple consumer instances and threads. By default, producers will distribute messages across partitions, though this can be customized with the partition selection and partition key families of configuration options (see link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-overview-configuring-output-bindings-partitioning[the documentation]). The producer `partitionCount` is irrelevant to this process, but is used to provision _new_ topics with a desired number of partitions (ignore this if you are manually creating topics on your broker). The following is a complete configuration for concurrent consumers:

[source,yml]
----
spring:
  cloud:
    stream:
      bindings:
        producer-out-0:
          destination: topicname
          # Used to privision new topics. Omit if topics are manually
          # configured.
          partitionCount: 3
        consumer-in-0:
          destination: topicname
          consumer:
            concurrency: 2
    function:
      definition: producer;consumer
----

=== Fault Tolerance

==== Redelivery

In the event of broker failure or application disconnect, recovery falls back on Kafka consumer offsets, which we will summarize based on <<Kafka, [1, pp. 97-99]>> in this paragraph. Message consumers do not ACK each individual message like is typical for JMS. Instead, consumers have an _offset_ for each partition which tracks the index of the last retrieved message. Consumers periodically _commit_ their current offsets for their partitions to the broker, which saves them persistently. At any time, a consumer's offset may be behind or ahead of the offset of their last successfully _processed_ message, depending on the commit strategy. After a crash or rebalance (the assignment of partitions to consumers which happens as consumers connect and disconnect), consumers retrieve offsets for their partitions from the broker, then start consuming messages. If the last committed offset is _behind_ the most recently retrieved message for a partition (that is, a consumer received some messages but had not yet committed an updated offset prior to a crash), then the broker will re-deliver some messages (the already-delivered message will be delivered again). If the last committed offset was _ahead_ of their last retrieved message (that is, the consumer committed an offset before retrieving messages up to that offset, then crashed), then those messages will never be delivered.

S.C.S. handles offset commits on our behalf. By default, S.C.S. commits offsets after the current batch of retrieved messages is processed. This is governed by the `autoCommitOffset` and `ackEachRecord` parameters. The `autoCommitOffset` defaults to `true`, instructing the framework to commit offsets after messages are processed. The `ackEachRecord` parameter defaults to `false`, instructing S.C.S. to commit offsets only after each _batch_ of messages has been processed rather than after each individual message. See link:https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#kafka-consumer-properties[Kafka consumer properties documentation for the Kafka binder] for more information.

NOTE: S.C.S. does not change the fact that duplicate message delivery is possible in the event of broker or application failure.

==== Retry

Message processing failures are handled at multiple levels. Consumer-level error handlers re-process a message multiple times, blocking the consumer thread, until a configured maximum number of attempts (if any). Once the consumer-level error handler is exhausted, the error propagates to the binder-level error handler. The binder-level error handler may re-deliver the failed message to the consumer some configurable number of times. Every time a message is redelivered by the binder-level error handler, the consumer-level error handler takes effect. If and when the binder is exhausted, the messaging system uses a configurable strategy to dispose of the unprocessable message.

By default, the Kafka binder error handler is the link:https://docs.spring.io/spring-kafka/docs/2.5.1.RELEASE/reference/html/#seek-to-current[Seek-to-current] error handler. This handler commits a new consumer offset whenever error are encountered so that the new offset points to the first erroneous message. This ensures that successfully processed messages are not re-delivered and re-processed. Failed messages, however, are retried immediately up to a maximum of ten times by default. The consumer-level error handler is a Retry Template with a maximum of 3 attempts including the first and a minimum delay of 1 second between each, a maximum delay of 10, and a back-off multiplier of 2.0 between each attempt. Finally, the default messaging-system-level error handler discards unprocessable entities.

Therefore, by default, every unprocessable entity may be delivered 3 times at the consumer level and ten times at the binder level for a maximum of 30 times. Since there is a 1s delay until the first re-delivery and a 2s delay to the next, each message takes approximately 3s, and so the whole process will tie a consumer down for around 90s, plus processing overhead, in the worst-case. Such a message is then "discarded" by the broker, which means (_to the best of my understanding_) that the consumer treats the message like a success and commits an offset beyond the erroneous message.

We have configured an "error prone" supplier and consumer processing chain which can be run with the `error-prone-numbers` profile. The configuration is located in the `application-error-prone-numbers.yml` file:

[source,yml]
----
include::src/main/resources/application-error-prone-numbers.yml[]
----

At the consumer level, we set `maxAttempts: 3` so that the consumer-level error handler will attempt to process any message at most 3 times before sending the error up to the binder. Since we have configured the consumer (under the kafka-specific settings) to use a dead-letter queue, the default seek-to-current error handler skips redelivery and instead flushes the message to the DLQ and moves on. As a result, every message is tried at most 3 times. Using default min, max, and backoff, this may take approximately 3s.

Our program prints the following output:

[source]
----
2021-08-22 20:59:24.814  INFO 34911 --- [container-0-C-1] c.g.t.silverbroccoli.ErrorProneHandlers  : Failed to process message: n=1 attempt=1
2021-08-22 20:59:25.817  INFO 34911 --- [container-0-C-1] c.g.t.silverbroccoli.ErrorProneHandlers  : Failed to process message: n=1 attempt=2
2021-08-22 20:59:27.818  INFO 34911 --- [container-0-C-1] c.g.t.silverbroccoli.ErrorProneHandlers  : Failed to process message: n=1 attempt=3
2021-08-22 20:59:27.821 ERROR 34911 --- [container-0-C-1] o.s.integration.handler.LoggingHandler   : org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.cloud.stream.function.FunctionConfiguration$FunctionToDestinationBinder$1@2bc76821]; nested exception is java.lang.RuntimeException: Kaboom! Failed to process message: n=1, failedMessage=GenericMessage [payload=byte[1], headers={skip-input-type-conversion=false, kafka_offset=1, scst_nativeHeadersPresent=true, kafka_consumer=org.apache.kafka.clients.consumer.KafkaConsumer@4b4c13f0, deliveryAttempt=3, kafka_timestampType=CREATE_TIME, kafka_receivedPartitionId=0, contentType=application/json, kafka_receivedTopic=error-prone-numbers, kafka_receivedTimestamp=1629677429327, kafka_groupId=group1}]
----

This error message is enqueued to `error.error-prone-numbers.group1`, which we can consume from in the usual way. We have configured a debug consumer to confirm, which shows the DQL `toString()` output for message number 1 (`payload=1`) at offset 1 (`kafka_offset=1`):

[source]
----
Found DLQ message: message=GenericMessage [payload=1, headers={x-original-offset=[B@2d84d106, x-original-partition=[B@cba706f, deliveryAttempt=1, kafka_timestampType=CREATE_TIME, kafka_receivedTopic=error.error-prone-numbers.group1, kafka_offset=1, x-exception-message=[B@4f07fe6a, x-exception-fqcn=[B@797d017e, scst_nativeHeadersPresent=true, kafka_consumer=org.apache.kafka.clients.consumer.KafkaConsumer@4c9ce7c7, x-original-topic=[B@72faf3b6, x-original-timestamp-type=[B@2d1ccc8e, id=2a01e567-093b-f13e-c443-944ebe37d7de, kafka_receivedPartitionId=0, contentType=application/json, x-original-timestamp=[B@5d1883e9, kafka_receivedTimestamp=1629681376203, kafka_groupId=anonymous.a8095808-e798-4f24-9c2e-fc417f483b4c, x-exception-stacktrace=[B@4c7f0fc5, timestamp=1629681376262}]
----

Note that Kafka encodes values for DLQ entires as byte arrays. I am not sure at time of writing how to change this behavior.

=== Monitoring

Using Spring Boot Actuator we can view all of our bindings and their status through the actuator REST API. AT a minimum we need to add actuator and web dependencies to the pom, then set the following properties:

[source,yml]
----
management:
  endpoints:
    web:
      exposure:
        # Or '*' to enble everything.
        include: 'bindings'
  endpoint:
    health:
      # Optional, but informative.
      show-details: always
----

WHen we navigate to /actuator/health we can see the topics our application uses along with the Kafka listeners, their state, whether they are paused, and their group IDs.

We can also navigate to .actuator/bindings/ for a thorough overview of all Kafka producers and their effective configuration. We can also see which bindings are _pausable_. Bindings may be paused and resumed using POST requests like so:

[source,sh]
----
curl localhost:8081/actuator/bindings/consumer-in-0/ -H 'content-type: application/json' -d '{"state": "PAUSED"}'
----

For more information, see the link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#_health_indicator[health indicator documentation] and the link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#binding_visualization_control[binding visualization and control documentation]. Note that link:https://stackoverflow.com/a/68896149/4816074[at this time, control of producer bindings is not supported].

== References

. [[Kafka]] G. Shapira, T. Palino, R. Sivaram, and K. Petty, _Kafka: The Definitive Guide_, 2nd ed. Sebastopol, CA, U.S.A: O'Reilly, 2020,