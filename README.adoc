= Silver Broccoli: Kafka Evaluation

This project is an evaluation of Apache Kafka and supporting integration libraries based on their ability to create an application with all the capabilities listed below. Each integration library is given its own project folder and chapter in this document.

Requirements:

. Connect to the instance securely.
. Configure two topics (with partitions and replication settings).
. Produce one or more structured (i.e. from POJOs) messages to each topic on demand. (This should work for any arbitrary stimuli, not just a framework hook.)
. Consume messages from the topics
  .. concurrently (in a competing-consumer relationship)
  .. as well as in a pub-sub relationship,
  .. with bounded retry,
  .. and dead-lettering,
  .. such that one topic has priority over the other.
. Produce multiple messages as a result of processing to two different topics
  .. within a transaction
  .. which is synchronized (non-XA) with a database transaction (for an idempotent database operation).
. Inspect the dead-letter topic (DLT).
. Gather metrics suitable for monitoring (to include everything recommended by [<<Kafka>>, ch. 11]).
. Create a batch consumer.

== 1. Spring Cloud Stream

Progress so far:

* [x] Connect to the instance securely (using SASL and TLS).
* [x] Configure `input-high`, `input-low`, `input-high.DLT`, `input-low.DLT`, `left`, and `right` topics (with partitions, replication, and retention settings).
* [x] Produce one or more structured (e.g. JSON via Jackson) messages to `input-high` and `input-low` on demand. (This should work for any arbitrary stimuli, not just a framework hook.)
* [x] Consume messages from the topics
  ** [x] concurrently (in a competing-consumer relationship)
  ** [x] as well as in a pub-sub relationship (e.g. a Logger),
  ** [x] with bounded retry,
  ** [x] and dead-lettering,
  ** [ ] such that one topic has priority over the other.
* [x] Produce multiple messages to multiple topics as a result of processing one message.
  ** [x] within a transaction
  ** [x] which is synchronized (non-XA) with a database transaction (for an idempotent database operation).
* [x] Inspect the dead-letter topic (DLT).
* [x] Gather metrics suitable for monitoring (to include everything recommended by [<<Kafka>>, ch. 11]).
* [ ] Create a batch consumer.

Additional research questions:

* [ ] What is the default message key used for partition selection?
* [ ] How do you change the default encoding of DLQ entries? The default is byte-arrays for values.

=== Documentation Links

Refer to the following documentation:

* link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-reference[Spring Cloud Stream]
* link:https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_apache_kafka_binder[Apache Kafka Binder for Spring Cloud Stream]
* link:https://github.com/spring-cloud/spring-cloud-stream-samples[Sample S.C.S. projects]
* link:https://docs.spring.io/spring-kafka/docs/current/reference/html/[Spring Kafka]
* link:https://kafka.apache.org/documentation/#configuration[Kafka configuration properties] (see topic and producer configs).

=== Setup And Run

This project assumes you have an Apache Kafka instance already running, such as hosted at console.redhat.com/beta. We will integrate with the instance using SASL/PLAIN authentication protected by TLS. It also requires an instance of PostgreSQL, which we provide a docker-compose file for.

To begin, create a `config/application-local.yml` file. This file will hold custom environment configuration and secrets which we do not want to commit to the repository. This file is already git-ignored. Add the following required fields:

[source,yml]
----
env:
  kafka:
    bootstrapservers: <broker connection string, e.g. hostname:port>
    securityprotocol: SASL_SSL
    saslmechanism: PLAIN
    user: <service account user name>
    password: <service account password>
----

Then launch the application:

[source, shell]
----
cd spring-cloud-stream/
docker-compose up
SPRING_PROFILES_ACTIVE=local,sasl ./mvnw spring-boot:run
----

NOTE: We can set environment variables like `env.kafka.user=foo` instead of using the local profile. Either way is sufficient.

=== 1. SASL Authentication

We instruct Spring to connect with Kafka using SASL and TLS by setting the appropriate properties; see `application-sasl.yml`, which sets those properties. Furthermore, we must configure a `KafkaJaasLoginModuleInitializer` bean to configure our username and password in JAAS, which we do in `SaslConfiguration.java`.

Mentioned earlier, the `sasl` profile relies on properties set either in the `local` configuration or in the environment.

WARNING: At tine of writing, this is under-documented.

=== 2. Topic Configuration

Spring Cloud Stream can configure topics with properties under producer bindings, like so:

[source,yml]
----
spring:
  cloud:
    stream:
      bindings:
        producer-out-0:
          producer:
            partitionCount: 3 # Provision new topics with 3 partitions.
      kafka:
        bindings:
          producer-out-0:
            producer:
              topic.properties:
                retention.bytes: 10000 # Maximum messages to retain, in bytes
                retention.ms: 60000    # How long to retain messages for, in ms
----

Some properties are kafka-specific, however, which is why we have both `s.c.s.bindings` and `s.c.s.kafka.bindings` in this example. Note that multiple bindings could produce to the same topic, but topic configuration should be defined only once. This said, we prefer to use the `TopicBuilder` from Spring Kafka to register new topics as `@Bean`s. This has the advantage of placing all properties in the same place, and avoids the multiple-bindings issue.

See `Topics.java` for an example:

[source, java]
----
@Bean
  public NewTopic inputHighTopic() {
    return TopicBuilder.name("input-high")
        .partitions(3)
        .replicas(2)
        .config(TopicConfig.RETENTION_MS_CONFIG, "60000")
        .build();
  }
----

WARNING: If the name of the bean (which is the function name by default) matches the name of the topic, Spring will fail with errors stating that the topic bean's type was NewTopic when SubscribableChannel was expected. This is not documented. Spring appears to mistake the `@Bean` for an internal representation of the topic with the same name.

=== 3. Producing Structured Messages

We use `StreamBridge` to produce messages to any topic at any time, such as on application startup or in response to HTTP requests. S.C.S. automatically JSON-serializes objects using Jackson; no configuration is required.

WARNING: If we define a POJO without a default constructor (or possible without other fields required for Jackson serialization), Spring will not raise Jackson serialization errors. It instead fails to rectify the `byte[]` format used by Kafka with the Pojo type. This is not documented.

=== 4. Consume Messages

We consume messages from a topic by registering a `Consumer<T>` bean and configuring a binder for the consumer. Consider the following:

[source, java]
----
@Bean
public Consumer<String> myConsumer() {
  return x -> {
    LOGGER.info("Got a message: " + x);
  }
}
----

[source, yaml]
----
spring:
  cloud:
    stream:
      bindings:
        myConsumer-in-0:
          destination: myTopic
    function:
      definition: myConsumer
----

The `myConsumer-in-0` binding configures a binding between our consumer bean named `myConsumer` and the `myTopic` topic. The `function.definition` configuration tells Spring Cloud Function (used by S.C.S) which function bindings are message handlers. When there is only one such binding, this may be elided.

==== 4.a. Competing Concurrent Consumers

We can start multiple instances of a consumer by setting the `s.c.s.bindings.<binding>.consumer.group` and `concurrency` configurations. The group sets these consumers to be members of the same consumer group (in a competing-consumer relationship), and the `concurrency` sets the number of consumers to start. Consumers are automatically assigned partitions.

NOTE: The concurrency setting configures the number of consumers _for each_ destination. If a binding subscribes to multiple topics (e.g. `s.c.s.bindings.<binding>.destination=foo,bar`), then a concurrency of `2` starts two consumers for every single destination. If any topic in the list of destinations has fewer partitions than the desired number of consumers, some consumers will simply be idle -- Spring still creates them.

We can confirm that our consumers are concurrent by looking at the name of consumer threads in the logs.

==== 4.b. Pub-Sub Concurrent Consumers

Function bindings in different `group`s do not compete for messages. Consumer groups each get a copy of every message from every partition they subscribe to.

==== 4.c & 4.d. Bounded Retry and Dead Lettering

We can configure Spring's Kafka Listener Container, which is automatically configured with defaults by Spring, to limit redeliveries and publish unprocessable entities to a DLT after exhausting all attempts. See the `Container.java` configuration for an example.

==== 4.e Prioritized Message Consumption.

TODO.

=== 5. Publish to Multiple Topics as a Result of Processing

S.C.S. functions can not, at this time, produce to multiple topics at once. Instead, we have to use `Consumer`s who produce messages via a `StreamBridge` instance See the `consumer` bean in `Bindings` for an example.

==== 5.a. Transacted Publishers

We can configure Spring's Kafka Listener Container, which is automatically configured with defaults by Spring, to create transactional producers for all bindings by using the following application configuration:

[source, yaml]
----
spring.cloud.stream.kafka:binder:
          transaction.transaction-id-prefix: 'tx-'
          required-acks: all
----

The transaction-id prefix is used to persistently identify producers (which makes duplicate messages identifiable). The required-acks settings forces producers to wait for all brokers to replicate each message.

Consumers do not respect transaction boundaries by default. We enable transaction isolation on the consumer side as follows:

[source, yaml]
----
spring.cloud.stream.bindings:
  myBinding-in-0:
    consumer.properties:
      isolation.level: read_committed
----

Such a consumer will only read messages from committed transactions (up to the first uncommitted transaction in a partition, known as the Last Stable Offset or LSO).

==== 5.b. Synchronizing Kafka transactions with Database Transactions

We can configure Kafka to commit two separate transactions one after the other. With transactional producers configured, all we have to do next is add a `@transactional` annotation to our unit of work to wrap it in a database transaction as well. Consider the following:

[source, java]
----
@Bean
public Consumer<Event> consumer(TransactedUnitOfWork work) {
  return work::run;
}

@Component
public static class TransactedUnitOfWork {

  private final StreamBridge bridge;
  private final AuditLogRepository repository;

  @Autowired
  public TransactedUnitOfWork(StreamBridge bridge, AuditLogRepository repository) {
    this.bridge = bridge;
    this.repository = repository;
  }

  @Transactional
  public void run(Event event) {
    bridge.send("left", new Event().message("LEFT: " + event.getMessage()));
    repository.createIfNotExists(event.getMessage());

    // Simulate a failure to exercise transactions.
    if (event.getMessage().startsWith("FAIL")) {
      throw new RuntimeException("Kaboom! Can't process event=" + event);
    }

    bridge.send("right", new Event().message("RIGHT: " + event.getMessage()));
  }
}
----

Because Spring uses proxies ("interceptors") to start transactions around method calls, we _have_ to register our unit of work as a bean rather than define it as a lambda. Spring has no way to decorate a S.C.Function lambda with a transaction interceptor. As a result, this function binding hardly seems idiomatic, and is definitely unintuitive.

That said, we must be mindful that Kafka _does not_ support XA. In the example above, Spring will try to commit the database transaction first and the kafka transaction second (this is the default). If the database operation fails to commit, both it and the kafka transaction will roll back. If the kafka transaction fails to commit, however, the database transaction _is not_ rolled back. Kafka may redeliver the message in this case, so we may end up trying to persist the same data twice. We should design our database operations to be idempotent as a result.

=== 6. Inspect a DLT

Dead-letter topics are normal topics. We subscribe to DLTs the same as any other.

=== 7. Monitoring & Metrics

Using Spring Boot Actuator we can monitor and control our system through the REST API. At a minimum we need to add actuator and web dependencies to the pom, then set the following properties:

[source,yml]
----
management:
  endpoints:
    web:
      exposure:
        # Or '*' to enble everything.
        include: 'bindings'
  endpoint:
    health:
      # Optional, but informative.
      show-details: always
----

When we navigate to /actuator/health we can see the topics our application uses along with the Kafka listeners, their state, whether they are paused, and their group IDs. For more information, see the link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#_health_indicator[health indicator documentation].

The /actuator/bindings api gives an overview of effective binding configuration and status. We can  also issue POST requests to control (start, stop, pause, and unpause) those bindings. Bindings which support pausing have the `pausable` flag set to true. For example, the following curl request will pause a consumer:

[source,sh]
----
curl localhost:8081/actuator/bindings/consumer-in-0/ -H 'content-type: application/json' -d '{"state": "PAUSED"}'
----

NOTE: At time of writing, link:https://stackoverflow.com/a/68896149/4816074[control of producer bindings is not supported].

For more information, see the link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#binding_visualization_control[binding visualization and control documentation].

Browse to `/actuator/metrics` for a long list of metrics collected by default. For example, `/actuator/metrics/spring.cloud.stream.binder.kafka.offset` measures _consumer lag_, or the number of messages which have been produced to a topic but not yet consumed by a consumer group [<<Kafka>>, pp. 316]. We can use the `?tag=KEY:VALUE` query parameter to filter results down to a specific topic and/or consumer gorup. For example, `/actuator/metrics/spring.cloud.stream.binder.kafka.offset?tag=topic:mytopic&tag=group:mygroup` shows the lag on the `mytopic` topic for the `mygroup` consumer group.

In the following two sections we will review the suggested metrics from [<<Kafka>>, ch. 11]. Bear in mind that most metrics endpoints can be filtered by multiple tags at once for better granularity.

==== For the producer

* The _record-error-rate_ metric is the rate at which producers fail to produce messages to Kafka (which are therefore lost). We should generate an alert whenever this is nonzero. The metric is found at `/actuator/metrics/kafka.producer.record.error.rate`.

* The _request-latency-avg_ metric is the average amount of time that a produce request takes. We should generate an alert whenever this value is outside a measured normal for our application. The metric is found at `http://localhost:8081/actuator/metrics/kafka.producer.request.latency.avg`.

* The _request-rate_ metric is the number of produce requests sent to brokers per second. Every request contains one or more batches of messages. The _record-send-rate_ measures the number of messages produced per second as a result of these batches. Finally, the _outgoing-byte-rate_ is the number of bytes overall sent per second. These metrics are available at the following endpoints:
** /actuator/metrics/kafka.producer.request.rate
** /actuator/metrics/kafka.producer.record.send.rate
** /actuator/metrics/kafka.producer.outgoing.byte.rate

* The _request-size-avg_ metric describes the average size in bytes of each producer request, which may contain one or more batches for a single topic partition. The average size of each batch is measured by the _batch-size-avg_ metric. The average size of any individual message is measured by _record-size-avg_, again in bytes. The average number of records in a produce request is tracked by _records-per-request-avg_. The following endpoints expose these metrics:
** actuator/metrics/kafka.producer.request.size.avg
** actuator/metrics/kafka.producer.batch.size.avg
** actuator/metrics/kafka.producer.record.size.avg
** actuator/metrics/kafka.producer.records.per.request.avg

*  The _record-queue-time-avg_ is the average amount of time between when a record is ready to be sent to the broker and when it is actually produced, in milliseconds. For context, be aware that when a producer delivers messages to Kafka, it attempts to do so in large batches. Producers will wait until `batch.size` messages are ready to send before producing. Producers will only wait up to `linger.ms` milliseconds, however, at which point they will send whatever messages they already have. The _record-queue-time-avg_ can indicate when the `batch.size` and `linger.ms` configuration of the Kafka producers are effective. This metric is published at `/actuator/metrics/kafka.producer.record.queue.time.avg`.

==== For the consumer

The consumer metrics we are interested in are all actually "fetch manager" metrics.

* The _fetch-latency-avg_ metric measures how long fetch requests to the broker take, which we may be able to generate alerts from based on measured normal latency. This is impacted by the consumer properties `fetch.min.bytes` and `fetch.max.wait.ms`. Similar to how batches are sent to the broker, consumers wait until a certain number of bytes worth of messages are available to fetch all at once, or until a certain timeout, in order to better utilize network and broker resources. As a result, latency can vary depending on topic activity, which may hamper attempts at using this metric for alerting. This metric is exposed at `/actuator/metrics/kafka.consumer.fetch.manager.fetch.latency.avg`.

* We should avoid the _records-lag-max_ metric, which measures consumer lag _only_ for the single worst partition a consumer is working on. Spring provides an alternative, _kafka offset_, which measures the total number of unconsumed messages by topic and/or group. This metric is exposed at `/actuator/metrics/spring.cloud.stream.binder.kafka.offset`.

* The _records-consumed-rate_  measures the number of messages consumed per second by a client. Similarly, the _bytes-consumed-rate_ measure the total number of bytes consumed by a client. These are exposed by the following endpoints:
** /actuator/metrics/kafka.consumer.fetch.manager.records.consumed.rate
** /actuator/metrics/kafka.consumer.fetch.manager.bytes.consumed.rate

* The _fetch-rate_ metric measures the average number of fetch requests made by a consumer per second. The _fetch-size-avg_ measures the average number of bytes of those requests, and the _records-per-request-avg_ gives us the average number of messages per fetch. There _is not_ an equivalent to the producer record-size-average metric by default. The following endpoints expose these data:
** /actuator/metrics/kafka.consumer.fetch.manager.fetch.rate
** /actuator/metrics/kafka.consumer.fetch.manager.fetch.size.avg
** /actuator/metrics/kafka.consumer.fetch.manager.records.per.request.avg

=== 8. Batch Processing

TODO.

== Kafka Notes

=== Consumer Offsets

In the event of broker failure or application disconnect, recovery falls back on Kafka consumer offsets, which we will summarize based on [<<Kafka>>, pp. 97-99] in this paragraph. Message consumers do not ACK each individual message like is typical for JMS. Instead, consumers have an _offset_ for each partition which tracks the index of the last retrieved message. Consumers periodically _commit_ their current offsets for their partitions to the broker, which saves them persistently. At any time, a consumer's offset may be behind or ahead of the offset of their last successfully _processed_ message, depending on the commit strategy. After a crash or rebalance (the assignment of partitions to consumers which happens as consumers connect and disconnect), consumers retrieve offsets for their partitions from the broker, then start consuming messages. If the last committed offset is _behind_ the most recently retrieved message for a partition (that is, a consumer received some messages but had not yet committed an updated offset prior to a crash), then the broker will re-deliver some messages (the already-delivered message will be delivered again). If the last committed offset was _ahead_ of their last retrieved message (that is, the consumer committed an offset before retrieving messages up to that offset, then crashed), then those messages will never be delivered.

=== Transactions, Exactly Once Semantics (EOS), and Idempotence

Kafka supports _exactly once semantics_ (EOS) that ensure messages are produced and consumed exactly one time. To guarantee exactly-once, Kafka ensures producers are _idempotent_, and our applications will frequently use _transactions_ to guard against duplicate side effects. These three things are all different aspects of the same concept.

An _idempotent_ producer is one that cannot produce a duplicate message as a result of retry (such as to recover from network or broker failures). When idempotent producers start, they retrieve a unique identifier from a Kafka broker. When an idempotent producer produces a message, it adds its unique id and the sequence number of the message. Combined with the topic and partition name, these uniquely identify the message. By default, the broker keeps track of the last 5 message IDs received per partition, and producers maintain no more than 5 in-flight requests. Together these allow brokers to reject all duplicate messages they might receive. For more information, such as how failover and disconnect scenarios are managed, see [<<Kafka>>, ch. 8].

NOTE: EOS semantics prevent duplicate delivery caused by producer retry. They do not prevent our application from producing the same content multiple times. For this reason we should rely on the producer mechanism exclusively to manage retry. If our application can generate identical message contents (e.g. if two producers read data from the same data source at the same time), we will need to handle it ourselves.

Stream processing may rely on transactions to guarantee EOS. Transactional processing uses transactions in the familiar way to ensure that a group of actions succeeds or fails atomically. While idempotent producers have a unique producer id which can change when producers restart, transactional producers maintain a consistent producer id that may be used to identify the same producer between restarts. When transactional producers produce messages, they are saved to a partition just like normal messages. By default, consumers read all messages, including uncommitted ones. Consumers may be configured instead to only read _committed_ messages. In this case they rely on the broker to track transaction boundaries within a partition, and in particular to track the position of the _last stable offset (LSO)_, the offset of the first still-open transaction in the partition. Consumers will _only_ consume messages from a partition up to the LSO. If any transaction is uncommitted, it prevents consumers from fetching any later messages, including non-transacted and committed messages, from the same partition. By default, transactions time out after 15 minutes.

WARNING: Kafka _does not_ support XA transactions. In other words, if a message handler successfully commits a database record in a transaction but does not successfully commit a message to Kafka, the database transaction _will not roll back._ For a message producer, this may simply mean our application needs to re-generate the message contents. For a transform function in the middle of a pipeline, though, this may lead to re-delivered messages. We are therefore advised to make all our database operations _idempotent_. When Kafka re-delivers the message our transform failed to consume fully, an idempotent database operation will amount to a no-op if it succeeded the last time we received the message, and so there is no harm done. It is possible, however, that a message may be chronically unprocessable. We may need a way to remove the orphaned database entry (perhaps as a consequence of DLQ processing).

== 2. Kafka Client

Progress so far:

* [x] Connect to the instance using OAUTH and TLS.
* [x] Configure `input-high`, `input-low`, `input-high.DLT`, `input-low.DLT`, `left`, and `right` topics (with partitions, replication, and retention settings).
* [x] Produce one or more structured (e.g. JSON via Jackson) messages to `input-high` and `input-low` on demand. (This should work for any arbitrary stimuli, not just a framework hook.)
* [ ] Consume messages from the topics
** [ ] concurrently (in a competing-consumer relationship)
** [ ] as well as in a pub-sub relationship (e.g. a Logger),
** [ ] with bounded retry,
** [ ] and dead-lettering,
** [ ] such that one topic has priority over the other.
* [ ] Produce multiple messages to multiple topics as a result of processing one message.
** [ ] within a transaction
** [ ] which is synchronized (non-XA) with a database transaction (for an idempotent database operation).
* [ ] Inspect the dead-letter topic (DLT).
* [ ] Gather metrics suitable for monitoring (to include everything recommended by [<<Kafka>>, ch. 11]).
* [ ] Create a batch consumer.

[#2-setup-and-run]
=== Setup And Run

This project assumes you have an Apache Kafka instance already running, such as hosted at console.redhat.com/beta. We will integrate with the instance using SASL/OAUTH authentication protected by TLS.

To begin, create a `config/application-local.yml` or `application-local.yml` file in the `kafka-client` directory. This file will hold custom environment configuration and secrets which we do not want to commit to the repository. This file is already git-ignored. Add the following required fields:

[source,yml]
----
sb:
  kafka-client:
    bootstrap.servers: TODO
    security.protocol: SASL_SSL
    sasl:
      mechanism: OAUTHBEARER
      login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler
      jaas.config: |
        org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required
        oauth.client.id=TODO
        oauth.client.secret=TODO
        oauth.token.endpoint.uri=TODO
        ;
----

Make sure to replace all the TODO's with real values, then launch the application:

[source, shell]
----
SPRING_PROFILES_ACTIVE=local,sasl ./mvnw spring-boot:run
----

NOTE: We can set environment variables like `sb.kafka-client.foo=bar` instead of using the local profile. Either way is sufficient.

=== 1. SASL Authentication

We configure the Kafka client to connect securely using SSL and SASL/OAUTHBEARER by setting the properties in the application-local.yml file, whose values are passed along to the KafkaProducer and KafkaConsumer instances (See <<2-setup-and-run,Setup And Run>>).

=== 2. Configure Topics

We can create and delete topics using `AdminClient#deleteTopics` and `AdminClient#createTopics` respectively. The `NewTopic` type allows us to configure the name, replication factor, and number of partitions for each topic, as well as set other topic properties using the `config` method. The `TopicConfig` class defines constants for topic configuration names.

[source, java]
----
adminClient.deleteTopics(
    List.of(
        "foo-topic" // topic name
));

adminClient.createTopics(
    List.of(
        new NewTopic(
            "foo-topic",      // topic name
            Optional.empty(), // use the broker's default number of partitions
            Optional.empty()  // use the broker's default replication factor
        ).config(Map.of(
            RETENTION_MS_CONFIG, "60000",  // keep messages for 1 minute
            RETENTION_BYTES_CONFIG, "1024" // keep one kebibyte of messages
))));
----

=== 3. Produce Structured Messages

Kafka producers communicate messages to the broker as byte arrays. Java objects are converted to byte array format based on the configured `key.serializer` and `value.serializer` classes, which implement the `org.apache.kafka.common.serialization.Serializer` interface [<<Kafka>>, pp. 50]. The kafka client comes with a few default implementations, such as the `StringSerializer` for `String` messages [<<Kafka>>, pp. 50], but for more complex data types we must write our own. For this evaluation we wrote a simpler serializer which writes arbitrary Object instances to byte arrays using the Jackson `ObjectMapper` API. See `com.github.tomboyo.silverbroccoli.kafka.JacksonObjectSerializer` in the `kafka-client` module.

When configuring producers, take advantage of the `ProducerConfig` and `CommonClientConfig` classes which define constants for property names.

== References

. [[Kafka,1]] G. Shapira, T. Palino, R. Sivaram, and K. Petty, _Kafka: The Definitive Guide_, 2nd ed. Sebastopol, CA, U.S.A: O'Reilly, 2020,