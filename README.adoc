= Silver Broccoli: Spring Cloud Stream (SCS) + Apache Kafka Evaluation

* [x] Document how to connect to a RHOAS Kafka instance.
* [x] Auto configure a topic.
* [x] Generate messages into the topic on a schedule.
* [x] Enqueue a message in response to arbitrary external stimuli.
* [x] Use a structured message type (A jackson-serialized POJO)
* [x] Enqueue a message as a consequence of processing another.
** [ ] Produce multiple messages.
* [x] Consume messages from one topic concurrently.
* [ ] Demonstrate re-tried message consumption.
* [ ] Demonstrate bounded re-try so that unprocessable entities do not block the topic.
* [ ] Create an idempotent message consumer
* [ ] Enable DLQ for a topic.
* [ ] Consume a batch of messages.
* [ ] Prioritize message consumption within a topic.

Additional questions:

* [ ] What is the default message key used for partition selection?

== Documentation Links

Refer to the following:

* link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-reference[Spring Cloud Stream]
* link:https://cloud.spring.io/spring-cloud-stream-binder-kafka/spring-cloud-stream-binder-kafka.html#_apache_kafka_binder[Apache Kafka Binder for Spring Cloud Stream]
* link:https://kafka.apache.org/documentation/#configuration[Kafka configuration properties] (see topic and producer configs).

== Development Setup

For local development you may either perform integrated testing in a docker-compose environment or with an externally deployed Kafka instance using the `sasl` profile.

=== Docker Compose
In this section we will launch our application on a private docker network along with zookeeper and kafka instances already completely configured.

To build and launch the application with docker-compose, run the following commands:

[source,sh]
----
./mvnw package && docker-compose build && docker-compose up
----

=== External Kafka Integration (With SASL Authentication)
In this section we will run the application from maven and integrate with an external kafka instance using SSL-encrypted SASL/PLAIN (username and password) authentication. We will inject SASL configuration into the application environment using an untracked application profile, but we could also set environment variables with the same names.

To begin, create a `config/application-local.yml` file. This file will hold local environemnt configuration and secreats which we do not want to commit to the repository, and as such is already git-ignored. Add the following required fields:

[source,yml]
----
env:
  kafka:
    bootstrapservers: <broker connection string, e.g. hostname:port>
    securityprotocol: SASL_SSL
    saslmechanism: PLAIN
    user: <service account user name>
    password: <service account password>
----

Now we launch the application with both the `sasl` and `local` profiles enabled. The `local` profile will simply read the application-local.yml file. The `sasl` profile maps those configurations to appropriate Kafka and SASL configurations.

NOTE: If we did not want to use an `application-local.yml` file, we could set environment properties of corresponding names (e.g. `env.kafka.bootstrapservers=localhost:1234`). In this case, we would not need to set the `local` profile.

To launch the application, invoke the following command:

[source,sh]
----
SPRING_PROFILES_ACTIVE=local,sasl ./mvnw spring-boot:run
----

== Evaluation Notes

=== Topic Configuration

We can manually set up topics on an external broker, or we can automatically provision them when the application starts. The latter is controlled by binding configurations like the following:

[source,yml]
----
spring:
  cloud:
    stream:
      bindings:
        producer-out-0:
          destination: numbers
          producer:
            # Provision new topics with 3 partitions. This configuration only
            # applies to producer bindings like this one.
            partitionCount: 3
      # Some producer configurations only apply to kafka, so they are are under
      # the s.c.s.kafka namespace.
      kafka:
        bindings:
          producer-out-0:
            producer:
              # Everything under this key is a native kafka configuration. See
              # below for a link to kafka.apache.org documentation. Do not
              # accidentally substitute colons for periods in these keys!
              topic.properties:
                # Retain only 10,000 bytes. The default is unlimited (-1).
                retention.bytes: 10000
                # Retain messages for 2 days. The default is 7. -1 is unlimited.
                retention.ms: 172800000
----

Kafka-specific property configuration may be found link:https://kafka.apache.org/documentation/#topicconfigs[here]. Note that new partitions can be added to existing topics, but is disabled by default (see kafka binder documentation for spring.cloud.stream.kafka.binder.autoAddPartitions).

=== Generate And Consume Messages

In this section we produce, transform, and consume messages using the native java functional types `Supplier`, `Function`, and `Consumer`. We will look at the more nuanced Reactor types `Flux` and `Mono` in a separate section.

To generate messages on a short fixed schedule (say, once a second), we can either configure a `Supplier<Flux<T>>` bean or just a `Supplier<T>` bean. The former must manually control the production rate, whereas the latter relies on S.C.S's global polling schedule (see link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#_polling_configuration_properties[polling configuration]). This application registers a `Supplier` to emit numbers to a topic. Find the relevant code in SilverBroccoliApplication.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/SilverBroccoliApplication.java[tag=generate-message-with-supplier]
----

To generate messages in response to arbitrary external events, we use an instance of  link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#_sending_arbitrary_data_to_an_output_e_g_foreign_event_driven_sources[StreamBridge]. StreamBridge allows us to enqueue messages one at a time to an arbitrary spring binding, which maps the message to a Kafka topic. This application serves a REST API (`POST /number`) which uses StreamBridge to enqueue a message in response to HTTP requests. Find the relevant code in NumberRestController.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/NumberRestController.java[tag=generate-message-with-streambridge]
----

To consume a message, we register an instance of `Consumer<IN>`. Find the relevant code in SilverBroccoliApplication.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/SilverBroccoliApplication.java[tag=consume-message-with-consumer]
----

To process one message and generate another as a result (that is, transform a message), we register an instance of `Function<IN, OUT>`. This is very similar to how we register producers and consumers. Find the relevant code in SilverBroccoliApplication.java below.

[source,java]
----
include::src/main/java/com/github/tomboyo/silverbroccoli/SilverBroccoliApplication.java[tag=transform-message-with-function]
----

=== Structured Messages

Spring Cloud Stream supports structures messages. We simply use Jackson-annotated POJOs as message objects. For an example, see NumberMessage.java and SilverBroccoliApplication.java.

=== Concurrent Message Consumers

In SilverBroccoliApplication.java, we have configured a `debugConsumer` message handler to consume from and display metadata about the `number-structured` topic. The topic has three partitions, and we start two consumer instances. The resulting output looks like the following:

----
2021-08-20 17:29:48.144  INFO 53572 --- [container-0-C-1] c.g.t.s.SilverBroccoliApplication        :
Partition=0
Thread=KafkaConsumerDestination{consumerDestinationName='numbers-structured', partitions=3, dlqName='null'}.container-0-C-1
ConsumerGroup=anonymous.f247e2f0-b12c-488e-9607-b940ef07d236
Payload=NumberMessage{ number=12 }

2021-08-20 17:29:49.145  INFO 53572 --- [container-0-C-1] c.g.t.s.SilverBroccoliApplication        :
Partition=1
Thread=KafkaConsumerDestination{consumerDestinationName='numbers-structured', partitions=3, dlqName='null'}.container-0-C-1
ConsumerGroup=anonymous.f247e2f0-b12c-488e-9607-b940ef07d236
Payload=NumberMessage{ number=13 }

2021-08-20 17:29:50.137  INFO 53572 --- [container-1-C-1] c.g.t.s.SilverBroccoliApplication        :
Partition=2
Thread=KafkaConsumerDestination{consumerDestinationName='numbers-structured', partitions=3, dlqName='null'}.container-1-C-1
ConsumerGroup=anonymous.f247e2f0-b12c-488e-9607-b940ef07d236
Payload=NumberMessage{ number=14 }
----

Above we see that the application (running in one JVM) has consumed three messages from the `numbers-structured` topic. They were consumed from the 0th, 1st, and 2nd partitions of the topic respectively. The thread is printed with each message. Since we're using the Kafka binder, threads are one-to-one with consumers, so since we can see only two threads (0-C-1 and 1-C-1), we know there are only two consumers drawing from all three partitions. This is expected: When a smaller number of consumers than partitions is configured, Kafka will balance some consumers to draw from multiple partitions. Note, however, that at most one consumer from a single consumer group may consume from any partition.

Concurrency configuration for Kafka is simple. The only required parameter is `consumer.concurrency`, which instructs S.C.S. to start multiple consumer instances and threads. By default, producers will distribute messages across partitions, though this can be customized with the partition selection and partition key families of configuration options (see link:https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-overview-configuring-output-bindings-partitioning[the documentation]). The producer `partitionCount` is irrelevant to this process, but is used to provision _new_ topics with a desired number of partitions (ignore this if you are manually creating topics on your broker). The following is a complete configuration for concurrent consumers:

[source,yml]
----
spring:
  cloud:
    stream:
      bindings:
        producer-out-0:
          destination: topicname
          # Used to privision new topics. Omit if topics are manually
          # configured.
          partitionCount: 3
        consumer-in-0:
          destination: topicname
          consumer:
            concurrency: 2
    function:
      definition: producer;consumer
----